{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction - Model Training\n",
    "## Building and Training the Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('house_data_cleaned.csv')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_model = df.copy()\n",
    "\n",
    "# Create new features\n",
    "df_model['Age'] = 2025 - df_model['YearBuilt']\n",
    "df_model['TotalRooms'] = df_model['Bedrooms'] + df_model['Bathrooms']\n",
    "df_model['Area_per_Room'] = df_model['Area'] / df_model['TotalRooms']\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"- Age (house age)\")\n",
    "print(\"- TotalRooms (bedrooms + bathrooms)\")\n",
    "print(\"- Area_per_Room\")\n",
    "\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df_model.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Label encoding for categorical variables\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"\\nEncoded {col}:\")\n",
    "    print(f\"Classes: {le.classes_}\")\n",
    "\n",
    "# Save label encoders for later use\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "print(\"\\nLabel encoders saved as 'label_encoders.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_model.drop('Price', axis=1)\n",
    "y = df_model['Price']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain-Test split ratio: 80-20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(\"Scaler saved as 'scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Train MSE': train_mse,\n",
    "        'Test MSE': test_mse,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MAE': test_mae,\n",
    "        'Train R²': train_r2,\n",
    "        'Test R²': test_r2\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"  MSE:  {train_mse:,.2f}\")\n",
    "    print(f\"  RMSE: {train_rmse:,.2f}\")\n",
    "    print(f\"  MAE:  {train_mae:,.2f}\")\n",
    "    print(f\"  R²:   {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"  MSE:  {test_mse:,.2f}\")\n",
    "    print(f\"  RMSE: {test_rmse:,.2f}\")\n",
    "    print(f\"  MAE:  {test_mae:,.2f}\")\n",
    "    print(f\"  R²:   {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# R² Score comparison\n",
    "axes[0, 0].bar(results_df.index, results_df['Test R²'], color=['blue', 'green', 'orange'])\n",
    "axes[0, 0].set_title('R² Score Comparison (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 1].bar(results_df.index, results_df['Test RMSE'], color=['blue', 'green', 'orange'])\n",
    "axes[0, 1].set_title('RMSE Comparison (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1, 0].bar(results_df.index, results_df['Test MAE'], color=['blue', 'green', 'orange'])\n",
    "axes[1, 0].set_title('MAE Comparison (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('MAE')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Train vs Test R² comparison\n",
    "x = np.arange(len(results_df.index))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, results_df['Train R²'], width, label='Train', color='lightblue')\n",
    "axes[1, 1].bar(x + width/2, results_df['Test R²'], width, label='Test', color='lightcoral')\n",
    "axes[1, 1].set_title('Train vs Test R² Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('R² Score')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(results_df.index)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Select Best Model and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on Test R² score\n",
    "best_model_name = results_df['Test R²'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print('='*60)\n",
    "print(f\"Test R² Score: {results_df.loc[best_model_name, 'Test R²']:.4f}\")\n",
    "print(f\"Test RMSE: {results_df.loc[best_model_name, 'Test RMSE']:,.2f}\")\n",
    "print(f\"Test MAE: {results_df.loc[best_model_name, 'Test MAE']:,.2f}\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'house_price_model.pkl')\n",
    "print(f\"\\nBest model saved as 'house_price_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Feature Importance (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If best model is tree-based, show feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"\\nFeature importance not available for Linear Regression\")\n",
    "    print(\"Showing coefficients instead:\")\n",
    "    \n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': best_model.coef_\n",
    "    }).sort_values('Coefficient', ascending=False)\n",
    "    \n",
    "    print(coefficients)\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coefficients, palette='coolwarm')\n",
    "    plt.title('Feature Coefficients - Linear Regression', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Create prediction dataframe\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Actual Price': y_test.values,\n",
    "    'Predicted Price': y_pred,\n",
    "    'Difference': y_test.values - y_pred,\n",
    "    'Absolute Error': np.abs(y_test.values - y_pred),\n",
    "    'Percentage Error': np.abs((y_test.values - y_pred) / y_test.values) * 100\n",
    "})\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(prediction_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Price', fontsize=12)\n",
    "axes[0].set_title('Actual vs Predicted Prices', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Price', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of errors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution of absolute errors\n",
    "axes[0].hist(prediction_df['Absolute Error'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Absolute Error', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Absolute Errors', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of percentage errors\n",
    "axes[1].hist(prediction_df['Percentage Error'], bins=30, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Percentage Error (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of Percentage Errors', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Save Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names for the app\n",
    "feature_names = list(X.columns)\n",
    "joblib.dump(feature_names, 'feature_names.pkl')\n",
    "print(f\"Feature names saved: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset: {len(df)} samples\")\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  R² Score: {results_df.loc[best_model_name, 'Test R²']:.4f}\")\n",
    "print(f\"  RMSE: ${results_df.loc[best_model_name, 'Test RMSE']:,.2f}\")\n",
    "print(f\"  MAE: ${results_df.loc[best_model_name, 'Test MAE']:,.2f}\")\n",
    "print(f\"\\nMean Absolute Percentage Error: {prediction_df['Percentage Error'].mean():.2f}%\")\n",
    "print(f\"\\nFiles Saved:\")\n",
    "print(\"  - house_price_model.pkl (trained model)\")\n",
    "print(\"  - scaler.pkl (feature scaler)\")\n",
    "print(\"  - label_encoders.pkl (categorical encoders)\")\n",
    "print(\"  - feature_names.pkl (feature names)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
